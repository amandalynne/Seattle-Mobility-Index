{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Geocoder Input \n",
    "This script performs a point in polygon inclusion test. We are testing whether or not the coordinates fall within the polygons. The output file lists the IDs of polygons that pass the test. This type of transormation can also be done using a spatial join tool in ArcGIS.\n",
    "\n",
    "This script takes GIS shapefiles as input and preprocesses them as key-value dictionaries where the key is the name of the geography and the value is a two dimensional list of vertices. This script also processes the shape data into a tableau-ready geospatial data format.\n",
    "\n",
    "Zip code, blockgroup, and council district shapefiles can be downloaded at the county GIS portal: https://www5.kingcounty.gov/gisdataportal/\n",
    "Informal neighorhoods can be downloaded at: https://data.seattle.gov/dataset/Neighborhoods/2mbt-aqqx\n",
    "Urban Villages can be downloaded at: https://data.seattle.gov/dataset/Urban-Villages/ugw3-tp9e\n",
    "\n",
    "The function geocode(float longtitude,float latitude) returns a list with the following values:\n",
    "\n",
    "    0 - Informal Neighborhood Short Name\n",
    "    1 - Informal Neighborhood Long Name\n",
    "    2 - Council District\n",
    "    3 - Zip Code\n",
    "    4 - Urban Village\n",
    "    5 - Block Group\n",
    "    6 - Geographical Area\n",
    "\n",
    "For information, contact Stephen Barham at stephen.barham@seattle.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.path as mplPath\n",
    "import numpy as np\n",
    "from matplotlib import path\n",
    "from ast import literal_eval\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import xmltodict\n",
    "try:\n",
    "    from urllib.request import Request, urlopen  # Python 3\n",
    "except:\n",
    "    from urllib2 import Request, urlopen  # Python 2\n",
    "    \n",
    "import pysal as ps\n",
    "\n",
    "import xml.etree.ElementTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create files from shapefiles. Note, this can take a long time.\n",
    "# we will call shapefileToDict(directory, inputfile, nameField, typeField, conversion=False) \n",
    "\n",
    "directory = \"V://Asset Management Program//Data Science//Geographies//\"\n",
    "\n",
    "# Blockgroup\n",
    "blockGroup_Dict = shapefileToDict(directory, 'blkgrp10_shore.shp', 'GEO_ID_GRP', 'LEVEL_3', True)\n",
    "w = csv.writer(open(directory + \"blkgrp10_shore.csv\", \"w\", newline=''))\n",
    "for key, val in blockGroup_Dict.items():\n",
    "    w.writerow([key, val])\n",
    "\n",
    "# Urban Village\n",
    "urbanVillage_Dict = shapefileToDict(directory, 'DPD_uvmfg_polygon.shp', 'UV_NAME', 'UV_TYPE', False)\n",
    "w = csv.writer(open(directory + \"DPD_uvmfg_polygon.csv\", \"w\", newline=''))\n",
    "for key, val in urbanVillage_Dict.items():\n",
    "    w.writerow([key, val])\n",
    "\n",
    "# Zip Code\n",
    "zipcode_Dict = shapefileToDict(directory, 'zipcode.shp', 'ZIPCODE', 'COUNTY', True)\n",
    "w = csv.writer(open(directory + \"Zipcode.csv\", \"w\", newline=''))\n",
    "for key, val in zipcode_Dict.items():\n",
    "    w.writerow([key, val])\n",
    "\n",
    "# Informal Neighborhood ShortName\n",
    "neighborhood_Dict = shapefileToDict(directory, 'Neighborhoods.shp', 'S_HOOD', 'L_HOOD', False)\n",
    "w = csv.writer(open(directory + \"Neighborhoods-Short.csv\", \"w\", newline=''))\n",
    "for key, val in neighborhood_Dict.items():\n",
    "    w.writerow([key, val])\n",
    "\n",
    "# Informal Neighborhood LongName\n",
    "neighborhood_Dict = shapefileToDict(directory, 'Neighborhoods.shp', 'L_HOOD', 'S_HOOD', False)\n",
    "w = csv.writer(open(directory + \"Neighborhoods-Long.csv\", \"w\", newline=''))\n",
    "for key, val in neighborhood_Dict.items():\n",
    "    w.writerow([key, val])\n",
    "\n",
    "# Council District\n",
    "councilDist_Dict = shapefileToDict(directory, 'sccdst.shp', 'SCCDST', 'NAME', True)\n",
    "w = csv.writer(open(directory + \"Sccdst.csv\", \"w\", newline=''))\n",
    "for key, val in councilDist_Dict.items():\n",
    "    w.writerow([key, val])\n",
    "df_Shapefile = ps.pdio.read_files(directory + 'sccdst.shp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Central Business District', 'DOWNTOWN', 'SCC7', 'Commercial Core')\n"
     ]
    }
   ],
   "source": [
    "## Load Data\n",
    "def geocode(lat,long):\n",
    "    \n",
    "    NeighborhoodShort = \"None\"\n",
    "    NeighborhoodLong = \"None\"\n",
    "    CouncilDistrict = \"None\"\n",
    "    ZipCode = \"None\"\n",
    "    UrbanVillage = \"None\"\n",
    "    BlockGroup = \"None\"\n",
    "\n",
    "    for key, value in csv.reader(open(directory + \"Neighborhoods-Short.csv\")):\n",
    "        p = path.Path(ast.literal_eval(value))\n",
    "        if p.contains_point((float(lat),float(long))) == True:\n",
    "            NeighborhoodShort = key.split(\"--\")[0]\n",
    "            \n",
    "    for key, value in csv.reader(open(directory + \"Neighborhoods-Long.csv\")):\n",
    "        p = path.Path(ast.literal_eval(value))\n",
    "        if p.contains_point((float(lat),float(long))) == True:\n",
    "            NeighborhoodLong = key.split(\"--\")[0]\n",
    "            \n",
    "    for key, value in csv.reader(open(directory + \"sccdst.csv\")):\n",
    "        p = path.Path(ast.literal_eval(value))\n",
    "        if p.contains_point((float(lat),float(long))) == True:\n",
    "            CouncilDistrict = key.split(\"--\")[0]\n",
    "    \n",
    "    for key, value in csv.reader(open(directory + \"DPD_uvmfg_polygon.csv\")):\n",
    "        p = path.Path(ast.literal_eval(value))\n",
    "        if p.contains_point((float(lat),float(long))) == True:\n",
    "            UrbanVillage = key.split(\"--\")[0]\n",
    "            \n",
    "    return (NeighborhoodShort, NeighborhoodLong, CouncilDistrict,UrbanVillage)\n",
    "\n",
    "print(geocode(47.606210,-122.332071))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD DICTIONARY FROM SHAPE FILES, CREATE TABLEAU MAP DATASET\n",
    "Build a dictionary from the KML file. This script also creates a tableau-formatted\n",
    "polygon file. Note, KML files exported from layers in ArcGIS may not work without additional processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a polygons dictionary from the KML file\n",
    "def shapefileToDict(directory, inputfile, nameField, typeField, conversion=False):\n",
    "    \n",
    "    polygons = {}\n",
    "    polygonCounts = {}\n",
    "    \n",
    "    # read shapefile via pysal\n",
    "    df_Shapefile = ps.pdio.read_files(directory + inputfile)\n",
    "    \n",
    "    #df_Shapefile =  df_Shapefile[df_Shapefile['GEO_ID_GRP'] == '530339901000']\n",
    "    #df_Shapefile =  df_Shapefile.iloc[0:2]\n",
    "    #print (df_Shapefile)\n",
    "    \n",
    "    # for tableau table construction\n",
    "    Longitude = []\n",
    "    Latitude = []\n",
    "    SortOrder = []\n",
    "    Location = []\n",
    "    polygonCount = 0\n",
    "    ID = []\n",
    "    Geography = \"\"\n",
    "    Name = \"\"\n",
    "    Type = []\n",
    "    \n",
    "    for index, row in df_Shapefile.iterrows():   \n",
    "        polygon = []\n",
    "        polygonXY = []\n",
    "\n",
    "        # check to make sure the name is not already in the dictionary    \n",
    "        if str(row[nameField]) in polygonCounts:\n",
    "            polygonCounts[str(row[nameField])] = polygonCounts[str(row[nameField])] + 1\n",
    "        else:\n",
    "            polygonCounts[str(row[nameField])] = 0 \n",
    "            \n",
    "        Name = row[nameField]\n",
    "        Geography = str(row[nameField])+ \"--\" + str(polygonCounts[row[nameField]])\n",
    "        sortOrder = 0\n",
    "        geo_type = \"\"\n",
    "        \n",
    "        for vertice in (row['geometry'].vertices):\n",
    "            \n",
    "            coordinate = []\n",
    "            status = \"open\"\n",
    "\n",
    "            # WGS Coordinates - no conversion\n",
    "            coordinate = (float(vertice[1]),float(vertice[0]))\n",
    "\n",
    "            # check to see if the polygon has closed\n",
    "            if coordinate in polygonXY: \n",
    "                status = \"closed\"\n",
    "            polygonXY.append(coordinate)\n",
    "                    \n",
    "            # State plane coordinates - Conversion\n",
    "            if conversion == True:\n",
    "                coordinate = (XYtoLatLong(float(vertice[0]), float(vertice[1])))\n",
    "            polygon.append(coordinate)\n",
    "                           \n",
    "            # for tableau\n",
    "            sortOrder = sortOrder + 1\n",
    "            Longitude.append(coordinate[1])\n",
    "            Latitude.append(coordinate[0])\n",
    "            ID.append(Geography)\n",
    "            SortOrder.append(sortOrder)\n",
    "            Location.append(Name)\n",
    "            Type.append(row[typeField])\n",
    "        \n",
    "            # add vertice centroids for polygon labels in Tableau!                      \n",
    "            if status == \"closed\": # This is a closed polygon, start a new one\n",
    "                polygons[Geography] = polygon\n",
    "                polygonCounts[str(row[nameField])] = polygonCounts[row[nameField]] + 1\n",
    "                Geography = str(row[nameField]) + \"--\" + str(polygonCounts[row[nameField]])\n",
    "                sortOrder = 0\n",
    "                polygon = []\n",
    "                polygonXY = []\n",
    "                geo_type = \"multi\"\n",
    "\n",
    "            else:\n",
    "                geo_type = \"\"\n",
    "                \n",
    "        if geo_type != \"multi\":\n",
    "            polygons[Geography] = polygon\n",
    "    \n",
    "    #Build table and export as csv        \n",
    "    dfOut = pd.DataFrame()\n",
    "    dfOut['id'] = ID\n",
    "    dfOut['lon'] = Longitude\n",
    "    dfOut['lat'] = Latitude\n",
    "    dfOut['sort_Order'] = SortOrder\n",
    "    dfOut['location'] = Location\n",
    "    dfOut['type'] = Type\n",
    "    \n",
    "    #dfOut.to_csv('V:\\Asset Management Program\\Data Science\\Geographies\\Tableau' + inputfile[:-4] + '.csv', mode='w', header=True, index=False, encoding='utf-8')\n",
    "    dfOut.to_csv('V://Asset Management Program//Data Science//Geographies//Tableau_' + inputfile[:-4] + '.csv', mode='w', header=True, index=False, encoding='utf-8')\n",
    "\n",
    "    return (polygons)\n",
    "    #print (dfOut)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point in Point Polygon Inclusion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XYtoLatLong(X, Y):\n",
    "\n",
    "    URL = \"http://citygis/coordinateconversion/coordinateconversion.asmx/WaStatePlaneToLatLong?X=\" + str(X) + \"&Y=\" + str(Y)\n",
    "    #q = Request(URL)\n",
    "    #data = urlopen(q).read()\n",
    "    #return (str(data['LatLongPoint']['Latitude']), str(data['LatLongPoint']['Longitude']))\n",
    "    \n",
    "    tree = ET.parse(urlopen(URL))\n",
    "    doc = tree.getroot()\n",
    "    return (float(doc[0].text), float(doc[1].text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://citygis/coordinateconversion/coordinateconversion.asmx/WaStatePlaneToLatLong?X=1260465.5400086045&Y=217803.2951618582\n",
      "None\n",
      "('47.58668', '-122.37308')\n"
     ]
    }
   ],
   "source": [
    "URL = \"http://citygis/coordinateconversion/coordinateconversion.asmx/WaStatePlaneToLatLong?X=1283237.4387780197&Y=268491.5308109835\"\n",
    "URL = \"http://citygis/coordinateconversion/coordinateconversion.asmx/WaStatePlaneToLatLong?X=1260465.5400086045&Y=217803.2951618582\"\n",
    "\n",
    "print (URL)     \n",
    "\n",
    "    \n",
    "import  xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse(urlopen(URL))\n",
    "doc = tree.getroot()\n",
    "print (doc.find('LatLongPoint'))\n",
    "\n",
    "print ((doc[0].text, doc[1].text))\n",
    "\n",
    "#q = Request(URL)\n",
    "#data = urlopen(q).read()\n",
    "\n",
    "#print (data)\n",
    "#print (str(data['LatLongPoint']['Longitude']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781868.29542933\n"
     ]
    }
   ],
   "source": [
    "directory = \"V://Asset Management Program//Data Science//Geographies//\"\n",
    "inputfile = \"blkgrp10_shore.shp\"\n",
    "\n",
    "df_Shapefile = ps.pdio.read_files(directory + inputfile)\n",
    "#print (df_Shapefile)\n",
    "print (df_Shapefile['geometry'][1421].area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
